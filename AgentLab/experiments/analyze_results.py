"""
ç»“æœåˆ†æå·¥å…· - WebArenaé£æ ¼æŠ¥å‘Š
=================================

åˆ†æå®éªŒç»“æœå¹¶ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
- æŒ‰éš¾åº¦åˆ†ç»„çš„æˆåŠŸç‡
- å¤±è´¥ä»»åŠ¡åˆ†æ
- æ­¥æ•°ç»Ÿè®¡
- å¯è§†åŒ–å›¾è¡¨

ç”¨æ³•:
    python experiments/analyze_results.py <result_dir>
    python experiments/analyze_results.py results/2025-12-13_18-00-00_*
"""

import sys
import json
from pathlib import Path
import pandas as pd
from typing import Dict, List, Optional
import argparse

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))


def load_experiment_results(result_dir: Path) -> Optional[pd.DataFrame]:
    """
    åŠ è½½å®éªŒç»“æœ
    
    Args:
        result_dir: ç»“æœç›®å½•è·¯å¾„
        
    Returns:
        DataFrame with results or None if not found
    """
    try:
        from agentlab.analyze import inspect_results
        df = inspect_results.load_result_df(result_dir)
        return df
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½ç»“æœ: {e}")
        return None


def load_task_metadata(task_ids: List[int]) -> Dict:
    """
    åŠ è½½ä»»åŠ¡å…ƒæ•°æ®
    
    Args:
        task_ids: ä»»åŠ¡IDåˆ—è¡¨
        
    Returns:
        Dict mapping task_id to task config
    """
    from benchmark.acidwave import AcidwaveBenchmark
    
    benchmark = AcidwaveBenchmark()
    task_map = {}
    
    for task in benchmark._tasks:
        task_map[task["task_id"]] = task
    
    return task_map


def extract_task_id(task_name: str) -> int:
    """ä»task_nameæå–task_id"""
    try:
        # Format: acidwave.task_<ID>
        return int(task_name.split("_")[-1])
    except:
        return -1


def analyze_by_difficulty(df: pd.DataFrame, task_map: Dict) -> pd.DataFrame:
    """æŒ‰éš¾åº¦åˆ†ç»„åˆ†æ"""
    
    # Add task metadata
    df['task_id'] = df['task_name'].apply(extract_task_id)
    df['difficulty'] = df['task_id'].apply(
        lambda tid: task_map.get(tid, {}).get('difficulty', 'unknown')
    )
    df['intent'] = df['task_id'].apply(
        lambda tid: task_map.get(tid, {}).get('intent', 'Unknown')
    )
    
    # Calculate success (reward > 0.8)
    df['success'] = df['cum_reward'] > 0.8
    
    # Group by difficulty
    difficulty_stats = []
    
    for diff in ['easy', 'medium', 'hard']:
        diff_tasks = df[df['difficulty'] == diff]
        if len(diff_tasks) == 0:
            continue
        
        stats = {
            'difficulty': diff,
            'total': len(diff_tasks),
            'success': diff_tasks['success'].sum(),
            'success_rate': diff_tasks['success'].mean() * 100,
            'avg_reward': diff_tasks['cum_reward'].mean(),
            'avg_steps': diff_tasks['n_steps'].mean(),
        }
        difficulty_stats.append(stats)
    
    return pd.DataFrame(difficulty_stats)


def analyze_failures(df: pd.DataFrame, task_map: Dict) -> List[Dict]:
    """åˆ†æå¤±è´¥ä»»åŠ¡"""
    
    # Add metadata
    df['task_id'] = df['task_name'].apply(extract_task_id)
    df['difficulty'] = df['task_id'].apply(
        lambda tid: task_map.get(tid, {}).get('difficulty', 'unknown')
    )
    df['intent'] = df['task_id'].apply(
        lambda tid: task_map.get(tid, {}).get('intent', 'Unknown')
    )
    
    # Find failures (reward <= 0.8)
    failures = df[df['cum_reward'] <= 0.8].copy()
    
    failure_list = []
    for _, row in failures.iterrows():
        failure_list.append({
            'task_id': row['task_id'],
            'task_name': row['task_name'],
            'difficulty': row['difficulty'],
            'intent': row['intent'],
            'reward': row['cum_reward'],
            'steps': row['n_steps'],
            'error': row.get('err_msg', 'No error message'),
        })
    
    return failure_list


def analyze_step_efficiency(df: pd.DataFrame, task_map: Dict) -> pd.DataFrame:
    """åˆ†ææ­¥æ•°æ•ˆç‡"""
    
    df['task_id'] = df['task_name'].apply(extract_task_id)
    df['difficulty'] = df['task_id'].apply(
        lambda tid: task_map.get(tid, {}).get('difficulty', 'unknown')
    )
    df['success'] = df['cum_reward'] > 0.8
    
    # Only analyze successful tasks
    success_df = df[df['success']].copy()
    
    if len(success_df) == 0:
        return pd.DataFrame()
    
    # Group by difficulty
    step_stats = []
    for diff in ['easy', 'medium', 'hard']:
        diff_tasks = success_df[success_df['difficulty'] == diff]
        if len(diff_tasks) == 0:
            continue
        
        stats = {
            'difficulty': diff,
            'avg_steps': diff_tasks['n_steps'].mean(),
            'min_steps': diff_tasks['n_steps'].min(),
            'max_steps': diff_tasks['n_steps'].max(),
            'std_steps': diff_tasks['n_steps'].std(),
        }
        step_stats.append(stats)
    
    return pd.DataFrame(step_stats)


def generate_report(result_dir: Path, output_file: Optional[Path] = None):
    """
    ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š
    
    Args:
        result_dir: ç»“æœç›®å½•
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ (None = æ‰“å°åˆ°æ§åˆ¶å°)
    """
    
    print("\n" + "="*80)
    print("ACIDWAVE å®éªŒç»“æœåˆ†æ")
    print("="*80)
    print(f"\nğŸ“ ç»“æœç›®å½•: {result_dir}")
    
    # Load results
    print("\n[1/5] åŠ è½½å®éªŒæ•°æ®...")
    df = load_experiment_results(result_dir)
    
    if df is None:
        print("âŒ æ— æ³•åŠ è½½ç»“æœ")
        return
    
    print(f"   âœ… åŠ è½½äº† {len(df)} ä¸ªä»»åŠ¡çš„ç»“æœ")
    
    # Load task metadata
    print("\n[2/5] åŠ è½½ä»»åŠ¡å…ƒæ•°æ®...")
    task_ids = df['task_name'].apply(extract_task_id).tolist()
    task_map = load_task_metadata(task_ids)
    print(f"   âœ… åŠ è½½äº† {len(task_map)} ä¸ªä»»åŠ¡çš„å…ƒæ•°æ®")
    
    # Overall statistics
    print("\n[3/5] è®¡ç®—æ•´ä½“ç»Ÿè®¡...")
    total = len(df)
    success_count = (df['cum_reward'] > 0.8).sum()
    partial_count = ((df['cum_reward'] > 0.3) & (df['cum_reward'] <= 0.8)).sum()
    fail_count = (df['cum_reward'] <= 0.3).sum()
    success_rate = (success_count / total * 100) if total > 0 else 0
    
    avg_reward = df['cum_reward'].mean()
    avg_steps = df['n_steps'].mean()
    
    # Difficulty analysis
    print("\n[4/5] æŒ‰éš¾åº¦åˆ†æ...")
    difficulty_df = analyze_by_difficulty(df, task_map)
    
    # Failure analysis
    print("\n[5/5] åˆ†æå¤±è´¥ä»»åŠ¡...")
    failures = analyze_failures(df, task_map)
    
    # Step efficiency
    step_efficiency_df = analyze_step_efficiency(df, task_map)
    
    # ==========================================
    # Generate Report
    # ==========================================
    
    report_lines = []
    
    def add_line(text=""):
        report_lines.append(text)
        print(text)
    
    add_line("\n" + "="*80)
    add_line("å®éªŒç»“æœè¯¦ç»†æŠ¥å‘Š")
    add_line("="*80)
    add_line(f"\nğŸ“Š æ€»ä½“è¡¨ç°")
    add_line("-" * 80)
    add_line(f"æ€»ä»»åŠ¡æ•°:      {total}")
    add_line(f"æˆåŠŸä»»åŠ¡:      {success_count:2d} ({success_rate:5.1f}%)")
    add_line(f"éƒ¨åˆ†å®Œæˆ:      {partial_count:2d}")
    add_line(f"å¤±è´¥ä»»åŠ¡:      {fail_count:2d}")
    add_line(f"å¹³å‡å¾—åˆ†:      {avg_reward:.3f}")
    add_line(f"å¹³å‡æ­¥æ•°:      {avg_steps:.1f}")
    
    # Difficulty breakdown
    add_line(f"\nğŸ“ˆ æŒ‰éš¾åº¦åˆ†ç»„")
    add_line("-" * 80)
    add_line(f"{'éš¾åº¦':<10} {'æ€»æ•°':<6} {'æˆåŠŸ':<6} {'æˆåŠŸç‡':<10} {'å¹³å‡åˆ†':<10} {'å¹³å‡æ­¥æ•°':<10}")
    add_line("-" * 80)
    
    for _, row in difficulty_df.iterrows():
        add_line(
            f"{row['difficulty']:<10} "
            f"{row['total']:<6.0f} "
            f"{row['success']:<6.0f} "
            f"{row['success_rate']:<10.1f}% "
            f"{row['avg_reward']:<10.3f} "
            f"{row['avg_steps']:<10.1f}"
        )
    
    # Step efficiency for successful tasks
    if len(step_efficiency_df) > 0:
        add_line(f"\nâš¡ æ­¥æ•°æ•ˆç‡ (ä»…æˆåŠŸä»»åŠ¡)")
        add_line("-" * 80)
        add_line(f"{'éš¾åº¦':<10} {'å¹³å‡':<8} {'æœ€å°‘':<8} {'æœ€å¤š':<8} {'æ ‡å‡†å·®':<8}")
        add_line("-" * 80)
        
        for _, row in step_efficiency_df.iterrows():
            add_line(
                f"{row['difficulty']:<10} "
                f"{row['avg_steps']:<8.1f} "
                f"{row['min_steps']:<8.0f} "
                f"{row['max_steps']:<8.0f} "
                f"{row['std_steps']:<8.2f}"
            )
    
    # Failed tasks
    if failures:
        add_line(f"\nâŒ å¤±è´¥ä»»åŠ¡è¯¦æƒ… ({len(failures)}ä¸ª)")
        add_line("-" * 80)
        
        for i, failure in enumerate(failures, 1):
            add_line(f"\n{i}. ä»»åŠ¡ {failure['task_id']} ({failure['difficulty']})")
            add_line(f"   ç›®æ ‡: {failure['intent'][:70]}")
            add_line(f"   å¾—åˆ†: {failure['reward']:.3f}")
            add_line(f"   æ­¥æ•°: {failure['steps']}")
            if failure['error'] and failure['error'] != 'No error message':
                add_line(f"   é”™è¯¯: {failure['error'][:100]}")
    
    # Recommendations
    add_line(f"\nğŸ’¡ æ”¹è¿›å»ºè®®")
    add_line("-" * 80)
    
    if success_rate < 50:
        add_line("âš ï¸  æˆåŠŸç‡è¾ƒä½ (<50%), å»ºè®®:")
        add_line("   1. æ£€æŸ¥agentæç¤ºè¯æ˜¯å¦æ¸…æ™°")
        add_line("   2. å¢åŠ max_steps (å½“å‰å¯èƒ½ä¸å¤Ÿ)")
        add_line("   3. ä½¿ç”¨ACIDWAVE_REASONING_AGENT")
        add_line("   4. æ£€æŸ¥Acidwaveåº”ç”¨æ˜¯å¦æ­£å¸¸è¿è¡Œ")
        add_line("   5. æŸ¥çœ‹å¤±è´¥ä»»åŠ¡çš„æˆªå›¾å’Œæ—¥å¿—")
    elif success_rate < 80:
        add_line("ğŸ”¶ æˆåŠŸç‡ä¸­ç­‰ (50-80%), å»ºè®®:")
        add_line("   1. é’ˆå¯¹å¤±è´¥ä»»åŠ¡ä¼˜åŒ–æç¤ºè¯")
        add_line("   2. è°ƒæ•´temperatureå‚æ•°")
        add_line("   3. æ”¹è¿›ä»»åŠ¡éªŒè¯é€»è¾‘")
        add_line("   4. åˆ†æå¤±è´¥æ¨¡å¼ (æ˜¯å¦é›†ä¸­åœ¨æŸä¸ªéš¾åº¦?)")
    else:
        add_line("âœ… æˆåŠŸç‡ä¼˜ç§€ (>80%)!")
        add_line("   å¯ä»¥å°è¯•:")
        add_line("   1. ä¼˜åŒ–æ­¥æ•°æ•ˆç‡ (å‡å°‘å†—ä½™æ“ä½œ)")
        add_line("   2. å¢åŠ æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡")
        add_line("   3. æµ‹è¯•å…¶ä»–æ¨¡å‹ (GPT-4o-mini)")
        add_line("   4. è¿›è¡Œæ¶ˆèå®éªŒ (ablation study)")
    
    # Next steps
    add_line(f"\nğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨")
    add_line("-" * 80)
    add_line(f"1. æŸ¥çœ‹è¯¦ç»†æ—¥å¿—:")
    add_line(f"   cd {result_dir}")
    add_line(f"   ls */experiment.log")
    add_line(f"\n2. æŸ¥çœ‹å¤±è´¥ä»»åŠ¡æˆªå›¾:")
    add_line(f"   cd {result_dir}")
    for failure in failures[:3]:  # Show first 3
        task_name = failure['task_name']
        add_line(f"   ls *{task_name}*/screenshot_*.png")
    
    add_line(f"\n3. ä½¿ç”¨AgentXrayå¯è§†åŒ–:")
    add_line(f"   agentlab-xray")
    
    add_line("\n" + "="*80)
    
    # Save to file
    if output_file:
        output_file = Path(output_file)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    else:
        # Save to default location
        default_output = result_dir / "analysis_report.txt"
        with open(default_output, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {default_output}")


def main():
    parser = argparse.ArgumentParser(
        description="åˆ†æAcidwaveå®éªŒç»“æœ (WebArenaé£æ ¼)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åˆ†ææœ€æ–°ç»“æœ
  python experiments/analyze_results.py results/2025-12-13_18-00-00_full_experiment
  
  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
  python experiments/analyze_results.py results/<dir> -o my_report.txt
  
  # åˆ†æå¤šä¸ªå®éªŒ (éœ€è¦æ‰‹åŠ¨è¿è¡Œå¤šæ¬¡)
  python experiments/analyze_results.py results/experiment1
  python experiments/analyze_results.py results/experiment2
        """
    )
    
    parser.add_argument(
        'result_dir',
        type=str,
        help='å®éªŒç»“æœç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '-o', '--output',
        type=str,
        help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ (é»˜è®¤: <result_dir>/analysis_report.txt)'
    )
    
    args = parser.parse_args()
    
    result_dir = Path(args.result_dir)
    
    if not result_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {result_dir}")
        sys.exit(1)
    
    generate_report(result_dir, args.output)


if __name__ == "__main__":
    main()



